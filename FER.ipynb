{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Rachit180/FER/blob/main/FER.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "uJrmKWUAyU_L",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 418
        },
        "outputId": "3255fd30-83ae-4a25-a8cd-5ba11e6e633a"
      },
      "outputs": [
        {
          "output_type": "error",
          "ename": "ImportError",
          "evalue": "cannot import name 'ImageDataGenerator' from 'keras.preprocessing.image' (/usr/local/lib/python3.10/dist-packages/keras/api/preprocessing/image/__init__.py)",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mImportError\u001b[0m                               Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-1-fa6ae20cb537>\u001b[0m in \u001b[0;36m<cell line: 15>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     13\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mkeras\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcallbacks\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mModelCheckpoint\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mEarlyStopping\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mtensorflow\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mkeras\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moptimizers\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mAdam\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mRMSprop\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mSGD\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mAdamax\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mNadam\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 15\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mkeras\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpreprocessing\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mimage\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mImageDataGenerator\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mload_img\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     16\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mkeras\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlayers\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mConv2D\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mMaxPool2D\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mFlatten\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mDense\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mDropout\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mBatchNormalization\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mMaxPooling2D\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mActivation\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mInput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mZeroPadding2D\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     17\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0msklearn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodel_selection\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mtrain_test_split\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mImportError\u001b[0m: cannot import name 'ImageDataGenerator' from 'keras.preprocessing.image' (/usr/local/lib/python3.10/dist-packages/keras/api/preprocessing/image/__init__.py)",
            "",
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0;32m\nNOTE: If your import is failing due to a missing package, you can\nmanually install dependencies using either !pip or !apt.\n\nTo view examples of installing some common dependencies, click the\n\"Open Examples\" button below.\n\u001b[0;31m---------------------------------------------------------------------------\u001b[0m\n"
          ],
          "errorDetails": {
            "actions": [
              {
                "action": "open_url",
                "actionText": "Open Examples",
                "url": "/notebooks/snippets/importing_libraries.ipynb"
              }
            ]
          }
        }
      ],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import seaborn as sns\n",
        "import keras\n",
        "from matplotlib import pyplot\n",
        "import matplotlib.pyplot as plt\n",
        "import tensorflow as tf\n",
        "\n",
        "from tensorflow.keras.utils import to_categorical\n",
        "from tensorflow.keras.models import Sequential\n",
        "from keras.callbacks import EarlyStopping\n",
        "from keras import regularizers\n",
        "from keras.callbacks import ModelCheckpoint, EarlyStopping\n",
        "from tensorflow.keras.optimizers import Adam, RMSprop, SGD, Adamax,Nadam\n",
        "from keras.preprocessing.image import ImageDataGenerator,load_img\n",
        "from keras.layers import Conv2D, MaxPool2D, Flatten, Dense, Dropout, BatchNormalization, MaxPooling2D, Activation, Input, ZeroPadding2D\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from keras.models import Model\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import accuracy_score\n",
        "from keras.regularizers import l1, l2\n",
        "from matplotlib import pyplot as plt\n",
        "from sklearn.metrics import confusion_matrix\n",
        "from sklearn.metrics import classification_report\n",
        "from imblearn.over_sampling import RandomOverSampler\n",
        "\n",
        "from tensorflow.keras.models import load_model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Pqrgj_cYzMk7"
      },
      "outputs": [],
      "source": [
        "import tensorflow as tf\n",
        "from tensorflow import keras\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "from sklearn.model_selection import train_test_split\n",
        "from keras.layers import Conv2D, MaxPool2D, AveragePooling2D, Input, BatchNormalization, MaxPooling2D, Activation, Flatten, Dense, Dropout\n",
        "from keras.models import Sequential\n",
        "from keras.utils import to_categorical\n",
        "from sklearn.metrics import classification_report\n",
        "from imblearn.over_sampling import RandomOverSampler\n",
        "from keras.preprocessing import image\n",
        "import scipy\n",
        "import os\n",
        "import cv2"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZecNn2aKzQVE"
      },
      "outputs": [],
      "source": [
        "data = pd.read_csv('/content/fer2013.csv')\n",
        "path='/content/fer2013.csv'"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "E0ZpPHgH0t-I"
      },
      "outputs": [],
      "source": [
        "data.head(5)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0Yjo1ECv8GIC"
      },
      "outputs": [],
      "source": [
        "# Split the pixel values into lists\n",
        "train_pixels = data['pixels'].str.split(\" \").tolist()\n",
        "\n",
        "# Find the maximum length of lists\n",
        "max_length = max(len(pixels) for pixels in train_pixels)\n",
        "\n",
        "# Pad the lists with zeros to make them of equal length\n",
        "train_pixels = [pixels + ['0'] * (max_length - len(pixels)) for pixels in train_pixels]\n",
        "\n",
        "# Convert the list of lists to numpy array of uint8\n",
        "train_pixels = np.uint8(train_pixels)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PMpBrFcV0w_p"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "\n",
        "# Iterate over the dataset to extract pixel values\n",
        "train_pixels = []\n",
        "max_length = 0  # Initialize maximum length to 0\n",
        "for index, row in data.iterrows():\n",
        "    pixels = row['pixels'].split(\" \")\n",
        "    pixels = [int(pixel) for pixel in pixels]\n",
        "    train_pixels.append(pixels)\n",
        "    if len(pixels) > max_length:\n",
        "        max_length = len(pixels)\n",
        "\n",
        "# Pad the lists with zeros to make them of equal length\n",
        "for pixels in train_pixels:\n",
        "    pixels.extend([0] * (max_length - len(pixels)))\n",
        "\n",
        "# Convert the list of lists to numpy array\n",
        "train_pixels = np.uint8(train_pixels)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XipU__jV1EDp"
      },
      "outputs": [],
      "source": [
        "def create(num):\n",
        "  image = train_pixels[num]\n",
        "  grid = image.reshape((48, 48))\n",
        "  plt.imshow(grid, cmap='viridis')\n",
        "  plt.title(data['emotion'][num])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pvysXpHs1Iuh"
      },
      "outputs": [],
      "source": [
        "create(33)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "H18tt5oQsItY"
      },
      "outputs": [],
      "source": [
        "x_data = data['pixels']\n",
        "y_data = data['emotion']\n",
        "\n",
        "y_data.value_counts()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2qLvmzBYsQb7"
      },
      "outputs": [],
      "source": [
        "from imblearn.over_sampling import SMOTE\n",
        "\n",
        "oversampler = RandomOverSampler(sampling_strategy='all')\n",
        "\n",
        "x_data, y_data = oversampler.fit_resample(x_data.values.reshape(-1,1), y_data)\n",
        "print(x_data.shape,\" \",y_data.shape)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FqWZ7sfwsT6e"
      },
      "outputs": [],
      "source": [
        "x_data = pd.Series(x_data.flatten())\n",
        "x_data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8FuiqgWysXGl"
      },
      "outputs": [],
      "source": [
        "x_data = np.array(list(map(str.split, x_data)), np.float32)\n",
        "x_data /= 255\n",
        "print(x_data[:10])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Enu9-IFnzAX6"
      },
      "outputs": [],
      "source": [
        "x_data = x_data.reshape(-1, 48, 48, 1)\n",
        "x_data.shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "N7QXX5tUzJtW"
      },
      "outputs": [],
      "source": [
        "y_data = np.array(y_data)\n",
        "print(y_data)\n",
        "y_data = y_data.reshape(y_data.shape[0], 1)\n",
        "y_data.shape\n",
        "print(y_data)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Sd81msZMzPwQ"
      },
      "outputs": [],
      "source": [
        "X_train, X_test, y_train, y_test = train_test_split(x_data, y_data, test_size=0.2, random_state=42)\n",
        "\n",
        "X_train, X_val, y_train, y_val = train_test_split(X_train, y_train, test_size=0.1, random_state=42)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CKNxFkNAzXrR"
      },
      "outputs": [],
      "source": [
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Conv2D, MaxPooling2D, Flatten, Dense,GlobalAveragePooling2D\n",
        "\n",
        "model = Sequential()\n",
        "\n",
        "# Input Layer\n",
        "model.add(Conv2D(32, kernel_size=(3, 3), input_shape=(48, 48, 1), activation='relu'))\n",
        "model.add(Conv2D(32, kernel_size=(3, 3), activation='relu'))\n",
        "model.add(MaxPooling2D(pool_size=(2, 2)))\n",
        "\n",
        "model.add(Conv2D(64, kernel_size=(3, 3), activation='relu'))\n",
        "model.add(Conv2D(64, kernel_size=(3, 3), activation='relu'))\n",
        "model.add(MaxPooling2D(pool_size=(2, 2)))\n",
        "\n",
        "model.add(Conv2D(64, kernel_size=(3, 3), activation='relu'))\n",
        "model.add(MaxPooling2D(pool_size=(2, 2)))\n",
        "\n",
        "model.add(GlobalAveragePooling2D())\n",
        "model.add(Dense(200, activation='relu'))\n",
        "model.add(Dense(7, activation='softmax'))\n",
        "\n",
        "model.summary()\n",
        "\n",
        "model.compile(\n",
        "    optimizer=Nadam(learning_rate=0.001),\n",
        "    loss='categorical_crossentropy',\n",
        "    metrics=['accuracy']\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7RzeSmN0zex-"
      },
      "outputs": [],
      "source": [
        "from tensorflow.keras import layers\n",
        "def double_conv_block(x, n_filters):\n",
        "   # Conv2D then ReLU activation\n",
        "    x = layers.Conv2D(n_filters, 3, padding = \"same\", activation = \"relu\", kernel_initializer = \"he_normal\")(x)\n",
        "#    # Conv2D then ReLU activation\n",
        "#     x = layers.Conv2D(n_filters, 3, padding = \"same\", activation = \"relu\", kernel_initializer = \"he_normal\")(x)\n",
        "    return x\n",
        "def downsample_block(x, n_filters):\n",
        "    f = double_conv_block(x, n_filters)\n",
        "    p = layers.MaxPool2D(2)(f)\n",
        "    p = layers.Dropout(0.3)(p)\n",
        "    return f, p\n",
        "def upsample_block(x, conv_features, n_filters):\n",
        "   # upsample\n",
        "    x = layers.Conv2DTranspose(n_filters, 3, 2, padding=\"same\")(x)\n",
        "   # concatenate\n",
        "    x = layers.concatenate([x, conv_features])\n",
        "   # dropout\n",
        "    x = layers.Dropout(0.3)(x)\n",
        "   # Conv2D twice with ReLU activation\n",
        "    x = double_conv_block(x, n_filters)\n",
        "    return x\n",
        "\n",
        "def build_unet_model():\n",
        "    inputs = layers.Input(shape=(48,48,1))\n",
        "   # encoder: contracting path - downsample\n",
        "   # 1 - downsample\n",
        "    f1, p1 = downsample_block(inputs, 8)\n",
        "   # 2 - downsample\n",
        "    f2, p2 = downsample_block(p1, 16)\n",
        "   # 3 - downsample\n",
        "    f3, p3 = downsample_block(p2, 32)\n",
        "   # 4 - downsample\n",
        "\n",
        "#     f4, p4 = downsample_block(p3, 64)\n",
        "\n",
        "   # 5 - bottleneck\n",
        "    bottleneck = double_conv_block(p3, 64)\n",
        "   # decoder: expanding path - upsample\n",
        "   # 6 - upsample\n",
        "#     u6 = upsample_block(bottleneck, f4, 64)\n",
        "   # 7 - upsample\n",
        "    u7 = upsample_block(bottleneck, f3, 32)\n",
        "   # 8 - upsample\n",
        "    u8 = upsample_block(u7, f2, 16)\n",
        "   # 9 - upsample\n",
        "    u9 = upsample_block(u8, f1, 8)\n",
        "\n",
        "   # outputs\n",
        "    gap = layers.GlobalAveragePooling2D()(u9)\n",
        "\n",
        "#     o = layers.Flatten()(u9)\n",
        "#     o1 = layers.Dense(10, activation = \"relu\")(o)\n",
        "    outputs = layers.Dense(7, activation = \"softmax\")(gap)\n",
        "#     outouts=layers.Flatten()\n",
        "   # unet model with Keras Functional API\n",
        "    unet_model = tf.keras.Model(inputs, outputs, name=\"U-Net\")\n",
        "\n",
        "    return unet_model"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from tensorflow.keras import layers\n",
        "\n",
        "def conv_block(x, n_filters, strides=(1, 1)):\n",
        "    shortcut = x\n",
        "\n",
        "    x = layers.Conv2D(n_filters, 3, strides=strides, padding='same', activation=None, kernel_initializer='he_normal')(x)\n",
        "    x = layers.BatchNormalization()(x)\n",
        "    x = layers.Activation('relu')(x)\n",
        "\n",
        "    x = layers.Conv2D(n_filters, 3, strides=(1, 1), padding='same', activation=None, kernel_initializer='he_normal')(x)\n",
        "    x = layers.BatchNormalization()(x)\n",
        "\n",
        "    if strides != (1, 1) or shortcut.shape[-1] != n_filters:\n",
        "        shortcut = layers.Conv2D(n_filters, 1, strides=strides, padding='valid', activation=None, kernel_initializer='he_normal')(shortcut)\n",
        "        shortcut = layers.BatchNormalization()(shortcut)\n",
        "\n",
        "    x = layers.Add()([x, shortcut])\n",
        "    x = layers.Activation('relu')(x)\n",
        "\n",
        "    return x\n",
        "\n",
        "def build_resnet_model():\n",
        "    inputs = layers.Input(shape=(48, 48, 1))\n",
        "\n",
        "    x = layers.Conv2D(64, 7, strides=(2, 2), padding='same', activation=None, kernel_initializer='he_normal')(inputs)\n",
        "    x = layers.BatchNormalization()(x)\n",
        "    x = layers.Activation('relu')(x)\n",
        "    x = layers.MaxPooling2D(pool_size=(3, 3), strides=(2, 2), padding='same')(x)\n",
        "\n",
        "    # Stage 1\n",
        "    x = conv_block(x, 64, strides=(1, 1))\n",
        "    x = conv_block(x, 64, strides=(1, 1))\n",
        "\n",
        "    # Stage 2\n",
        "    x = conv_block(x, 128, strides=(2, 2))\n",
        "    x = conv_block(x, 128, strides=(1, 1))\n",
        "\n",
        "    # Stage 3\n",
        "    x = conv_block(x, 256, strides=(2, 2))\n",
        "    x = conv_block(x, 256, strides=(1, 1))\n",
        "\n",
        "    # Stage 4\n",
        "    x = conv_block(x, 512, strides=(2, 2))\n",
        "    x = conv_block(x, 512, strides=(1, 1))\n",
        "\n",
        "    # Global Average Pooling\n",
        "    x = layers.GlobalAveragePooling2D()(x)\n",
        "\n",
        "    # Output\n",
        "    outputs = layers.Dense(7, activation='softmax')(x)\n",
        "\n",
        "    # ResNet model with Keras Functional API\n",
        "    resnet_model = tf.keras.Model(inputs, outputs, name='ResNet')\n",
        "\n",
        "    return resnet_model\n"
      ],
      "metadata": {
        "id": "WrSTFbxu14Cx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "resnet_model = build_resnet_model()\n",
        "resnet_model.summary()"
      ],
      "metadata": {
        "id": "Kw9HdCdu17mA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rFjbkXS_zvQd"
      },
      "outputs": [],
      "source": [
        "unet_model = build_unet_model()\n",
        "unet_model.summary()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rp2UFGjoz0SP"
      },
      "outputs": [],
      "source": [
        "tf.keras.utils.plot_model(unet_model, show_shapes=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "27gwrE1bz2c9"
      },
      "outputs": [],
      "source": [
        "nag = SGD(learning_rate=0.01, momentum=0.9, nesterov=True)\n",
        "unet_model.compile(optimizer=Nadam(learning_rate=0.005), loss='categorical_crossentropy', metrics=['accuracy'])\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2yQ3rovkz6_e"
      },
      "outputs": [],
      "source": [
        "print(y_train.shape)\n",
        "y_train = y_train.reshape((-1,1))\n",
        "print(y_train.shape)\n",
        "y_test = y_test.reshape((-1,1))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Y8Ia-O-rz9ng"
      },
      "outputs": [],
      "source": [
        "print(y_train.shape)\n",
        "y_train = to_categorical(y_train, 7)\n",
        "\n",
        "print(y_train.shape)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5Wx3DSKK0FHn"
      },
      "outputs": [],
      "source": [
        "y_test = to_categorical(y_test, 7)\n",
        "y_test.shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "iK-b1hog0Hmt"
      },
      "outputs": [],
      "source": [
        "y_val = to_categorical(y_val, 7)\n",
        "y_val.shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZJKVE2W80K2Z"
      },
      "outputs": [],
      "source": [
        "checkpointer = [EarlyStopping(monitor = 'val_accuracy', verbose = 1,\n",
        "                              restore_best_weights=True,mode=\"max\",patience = 10),\n",
        "                ModelCheckpoint('best_model.h5',monitor=\"val_accuracy\",verbose=1,\n",
        "                                save_best_only=True,mode=\"max\")]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nLBnkVp00Qfn"
      },
      "outputs": [],
      "source": [
        "history = unet_model.fit(X_train, y_train,\n",
        "                    epochs=60,\n",
        "                    batch_size=64,\n",
        "                    verbose=1,\n",
        "                    callbacks=[checkpointer],\n",
        "                    validation_data=(X_val, y_val))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "B3x7_6ac0Xp5"
      },
      "outputs": [],
      "source": [
        "history = model.fit(X_train, y_train,\n",
        "                    epochs=60,\n",
        "                    batch_size=64,\n",
        "                    verbose=1,\n",
        "                    callbacks=[checkpointer],\n",
        "                    validation_data=(X_val, y_val))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cgdweZMi0pOD"
      },
      "outputs": [],
      "source": [
        "print(\"Accuracy of our model on test data : \" , model.evaluate(X_test,y_test)[1]*100 , \"%\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XKyjporE0uIA"
      },
      "outputs": [],
      "source": [
        "model.save('fer_2013_74.60_resnet.keras')"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyPZyBYc9nP8W5JzVDzXjmz2",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}